{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Lecture @ Strive School - 21st July 2021\n",
    "# NER update\n",
    "\n",
    "'''\n",
    "Since today we are exploring the world of natural language processing, we’ll deepen in the Named Entity Recognition technique: this is just one of the mechanisms that NLP embodies. The recognition of named entities as the process of automatic identification of the entities present in a text and consequent classification into predefined categories such as \"person\", \"organization\", \"position\" is a quite common activity and expect for English, trained models with spaCy offer few labels that could be improved through training.\n",
    "\n",
    "Following the case study of this morning, try to emulate it in order to label all the brands present in the provided datasets, choosing the one you prefer OR trying to label all them and to train the model to recognize new different entities. The result should be twofold: the final model should be able to recognize brands that it has already seen, but already new ones.\n",
    "The brands proposed in the dataset concern fashion, cars and food.\n",
    "In order to test the accuracy of the model, test it with sentences and brands the model has never seen.\n",
    "\n",
    "Sample of the dataset\n",
    "---------------------\n",
    "- Cate Blanchett in Armani Privé. Rating: 8. Concludes as a rare butterfly, or from Rorschach's Test, or from computerized axial tomography.\n",
    "- I liked everything, recommend it! Another quality Xiaomi product...\n",
    "- What is the price of that Fiat 500XL?\n",
    "\n",
    "Info:\n",
    "- Feel free to change or arrange a new dataset\n",
    "- Try experimenting and tuning with the hyperparameters\n",
    "- Feel free to use or change the code you've seen during the morning session\n",
    "- TBD = To be done (from you!) :)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T12:07:32.209388Z",
     "start_time": "2021-07-23T12:07:32.200394Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 0 - PRE REQUISITES\n",
    "\n",
    "# python -m spacy download en_core_web_lg\n",
    "\n",
    "# TBD: Import libraries\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.training import Example\n",
    "\n",
    "# TBD: Load preferred model\n",
    "\n",
    "with open(\"food.txt\") as file:\n",
    "    dataset = file.read()\n",
    "\n",
    "# TBD: Load the dataset and test it as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T12:09:35.783404Z",
     "start_time": "2021-07-23T12:09:20.057936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: [('Italian', 'NORP'), ('first', 'ORDINAL'), ('Arrosticini', 'PERSON'), ('Alfredo', 'PERSON'), ('Naples', 'GPE'), ('Zaza', 'PERSON'), ('ApplePie', 'ORG'), ('Bologna', 'GPE'), ('Fiorentina Steak', 'PERSON'), ('Pineapple', 'ORG'), ('First', 'ORDINAL'), ('Bronte', 'PERSON'), ('Coca-cola', 'ORG'), ('Fanta', 'ORG'), ('Pepsi', 'ORG'), ('One', 'CARDINAL'), ('Sorrento', 'GPE'), ('Coffee\\nBread', 'ORG'), ('Love', 'WORK_OF_ART'), ('Fatte', 'PERSON'), ('vedrai che il mondo poi ti', 'PERSON'), ('Pastiera', 'PERSON'), ('the United States', 'GPE'), ('Two', 'CARDINAL'), ('two', 'CARDINAL'), ('24 hours', 'TIME'), ('24', 'CARDINAL'), ('two hours', 'TIME')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(dataset)\n",
    "print(\"Entities:\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 - TRAIN DATA\n",
    "\n",
    "# Prepare training data\n",
    "\n",
    "# TBD: define all the entities by extracting the words and their indexes from the dataset\n",
    "# expected format is the following:  (\"sentence\", {\"entities\": [0,10, \"FOOD\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T12:28:54.839308Z",
     "start_time": "2021-07-23T12:28:54.831312Z"
    }
   },
   "outputs": [],
   "source": [
    "words = [\"ketchup\", \"pasta\", \"carrot\", \"pizza\",\n",
    "         \"garlic\", \"tomato sauce\", \"basil\", \"carbonara\",\n",
    "         \"eggs\", \"cheek fat\", \"pancakes\", \"parmigiana\", \"eggplant\",\n",
    "         \"fettucine\", \"heavy cream\", \"polenta\", \"risotto\", \"espresso\",\n",
    "         \"arrosticini\", \"spaghetti\", \"fiorentina steak\", \"pecorino\",\n",
    "         \"maccherone\", \"nutella\", \"amaro\", \"pistachio\", \"coca-cola\",\n",
    "         \"wine\", \"pastiera\", \"watermelon\", \"cappuccino\", \"ice cream\",\n",
    "         \"soup\", \"lemon\", \"chocolate\", \"pineapple\"]\n",
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"food.txt\") as file:\n",
    "#     dataset = file.readlines()\n",
    "#     for sentence in dataset:\n",
    "#         print(\"######\")\n",
    "#         print(\"sentence: \", sentence)\n",
    "#         print(\"######\")\n",
    "#         sentence = sentence.lower()\n",
    "#         entities = []\n",
    "#         for word in words:\n",
    "#             word = word.lower()\n",
    "#             if word in sentence:\n",
    "#                 start_index = sentence.index(word)\n",
    "#                 end_index = len(word) + start_index\n",
    "#                 print(\"word: \", word)\n",
    "#                 print(\"----------------\")\n",
    "#                 print(\"start index:\", start_index)\n",
    "#                 print(\"end index:\", end_index)\n",
    "#                 pos = (start_index, end_index, \"FOOD\")\n",
    "#                 entities.append(pos)\n",
    "#         element = (sentence.rstrip('\\n'), {\"entities\": entities})\n",
    "#\n",
    "#         train_data.append(element)\n",
    "#         print('----------------')\n",
    "#         print(\"element:\", element)\n",
    "#\n",
    "#         # (\"this is my sentence\", {\"entities\": [0, 4, \"PREP\"]})\n",
    "#         # (\"this is my sentence\", {\"entities\": [6, 8, \"VERB\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - UPDATE MODEL\n",
    "\n",
    "# TBD: load the needed pipeline\n",
    "\n",
    "# TBD: define the annotations\n",
    "\n",
    "# TBD: train the model\n",
    "\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# TBD: define the number of iterations, the batch size and the drop according to your experience or using an empirical value\n",
    "# Train model\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    for iteration in range(0):\n",
    "        print(\"Iteration #\" + str(iteration))\n",
    "\n",
    "        # Data shuffle for each iteration\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in spacy.util.minibatch(train_data, size=3):\n",
    "            for text, annotations in batch:\n",
    "                # Create an Example object\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                # Update the model\n",
    "                nlp.update([example], losses=losses, drop=0.1)\n",
    "        print(\"Losses:\", losses)\n",
    "\n",
    "# Save the model\n",
    "# TBD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 - TEST THE UPDATED MODEL\n",
    "\n",
    "# Load updated model\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "\n",
    "# TBD: test with a old sentence\n",
    "print(\"entities:\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "# TBD: test with a new sentence and an old brand\n",
    "\n",
    "# TBD: test with a new sentence and a new brand"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
