{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "floating-fiction",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "funky-burning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T20:37:42.773547Z",
     "start_time": "2021-07-19T20:37:40.950840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import spacy\\n\\n%load_ext nb_black\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\";\n",
       "                var nbb_formatted_code = \"import spacy\\n\\n%load_ext nb_black\\n\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "%load_ext nb_black\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "closed-water",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T20:37:42.808528Z",
     "start_time": "2021-07-19T20:37:42.781541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\\ndoc = nlp(u\\\"Hello, world. Antonio is learning Python.\\\")\\ntype(doc)\";\n",
       "                var nbb_formatted_code = \"# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\\ndoc = nlp(u\\\"Hello, world. Antonio is learning Python.\\\")\\ntype(doc)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\n",
    "doc = nlp(u\"Hello, world. Antonio is learning Python.\")\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-covering",
   "metadata": {},
   "source": [
    "## Get tokens and sentences\n",
    "\n",
    "#### What is a Token?\n",
    "A token is a single chopped up element of the sentence, which could be a word or a group of words to analyse. The task of chopping the sentence up is called \"tokenisation\".\n",
    "\n",
    "Example: The following sentence can be tokenised by splitting up the sentence into individual words.\n",
    "\n",
    "\t\"Antonio is learning Python!\"\n",
    "\t[\"Antonio\",\"is\",\"learning\",\"Python!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "painted-oxford",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T20:37:42.934614Z",
     "start_time": "2021-07-19T20:37:42.811524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello, world.\n",
      "Antonio is learning Python.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# Get first token of the processed document\\ntoken = doc[0]\\nprint(token)\\n\\n# Print sentences (one sentence per line)\\nfor sent in doc.sents:\\n    print(sent)\";\n",
       "                var nbb_formatted_code = \"# Get first token of the processed document\\ntoken = doc[0]\\nprint(token)\\n\\n# Print sentences (one sentence per line)\\nfor sent in doc.sents:\\n    print(sent)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0192473a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T20:37:43.029785Z",
     "start_time": "2021-07-19T20:37:42.937609Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world.\n",
      "Antonio is learning Python.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"for sent in doc.sents:\\n    print(sent)\";\n",
       "                var nbb_formatted_code = \"for sent in doc.sents:\\n    print(sent)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-america",
   "metadata": {},
   "source": [
    "## Part of speech tags\n",
    "\n",
    "#### What is a Speech Tag?\n",
    "A speech tag is a context sensitive description of what a word means in the context of the whole sentence.\n",
    "More information about the kinds of speech tags which are used in NLP can be [found here](http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/).\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. CARDINAL, Cardinal Number - 1,2,3\n",
    "2. PROPN, Proper Noun, Singular - \"Jan\", \"Javier\", \"Antonio\", \"Italy\"\n",
    "3. INTJ, Interjection - \"Ohhhhhhhhhhh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regular-carolina",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T20:37:43.143331Z",
     "start_time": "2021-07-19T20:37:43.031767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello - INTJ\n",
      ", - PUNCT\n",
      "world - NOUN\n",
      ". - PUNCT\n",
      "Antonio - PROPN\n",
      "is - AUX\n",
      "learning - VERB\n",
      "Python - PROPN\n",
      ". - PUNCT\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# For each token, print corresponding part of speech tag\\nfor token in doc:\\n    print(\\\"{} - {}\\\".format(token, token.pos_))\";\n",
       "                var nbb_formatted_code = \"# For each token, print corresponding part of speech tag\\nfor token in doc:\\n    print(\\\"{} - {}\\\".format(token, token.pos_))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print(\"{} - {}\".format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "134d170f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T20:37:43.354737Z",
     "start_time": "2021-07-19T20:37:43.145328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello        INTJ       UH       interjection\n",
      ",            PUNCT      ,        punctuation mark, comma\n",
      "world        NOUN       NN       noun, singular or mass\n",
      ".            PUNCT      .        punctuation mark, sentence closer\n",
      "Antonio      PROPN      NNP      noun, proper singular\n",
      "is           AUX        VBZ      verb, 3rd person singular present\n",
      "learning     VERB       VBG      verb, gerund or present participle\n",
      "Python       PROPN      NNP      noun, proper singular\n",
      ".            PUNCT      .        punctuation mark, sentence closer\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"for word in doc:\\n    print(\\n        f\\\"{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}\\\"\\n    )\";\n",
       "                var nbb_formatted_code = \"for word in doc:\\n    print(\\n        f\\\"{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}\\\"\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for word in doc:\n",
    "    print(\n",
    "        f\"{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "english-economy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T20:37:43.505782Z",
     "start_time": "2021-07-19T20:37:43.356737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"from spacy import displacy\";\n",
       "                var nbb_formatted_code = \"from spacy import displacy\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-surgeon",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:40.970Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fb\\.conda\\envs\\nlp\\lib\\site-packages\\spacy\\displacy\\__init__.py:97: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e1171ea699ed4c26bb4030eab21efc8f-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Hello,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">world.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Antonio</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Python.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e1171ea699ed4c26bb4030eab21efc8f-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e1171ea699ed4c26bb4030eab21efc8f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M220.0,179.0 L228.0,167.0 212.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e1171ea699ed4c26bb4030eab21efc8f-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e1171ea699ed4c26bb4030eab21efc8f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e1171ea699ed4c26bb4030eab21efc8f-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e1171ea699ed4c26bb4030eab21efc8f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e1171ea699ed4c26bb4030eab21efc8f-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e1171ea699ed4c26bb4030eab21efc8f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-rabbit",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:40.974Z"
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-typing",
   "metadata": {},
   "source": [
    "We have said that dependency structures are represented by directed graphs that satisfy the following constraints:\n",
    "\n",
    "1. There is a single designated root node that has no incoming arcs.\n",
    "\n",
    "2. With the exception of the root node, each vertex has exactly one incoming arc.\n",
    "\n",
    "3. There is a unique path from the root node to each vertex in V.\n",
    "\n",
    "You can inspect the head of each token by invoking the `.head` attribute of a spaCy token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-violence",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:40.977Z"
    }
   },
   "outputs": [],
   "source": [
    "doc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-integration",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:40.979Z"
    }
   },
   "outputs": [],
   "source": [
    "doc[2].head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-character",
   "metadata": {},
   "source": [
    "So how would you search for the root?\n",
    "\n",
    "Since there is a unique path from the root node to each vertex in V, there's only one root node that has no incoming arcs, we can search for the token which have as head itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-armstrong",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:40.982Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.head == token:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-ground",
   "metadata": {},
   "source": [
    "As expected, since there were two sentences in the doc, we got two roots.\n",
    "\n",
    "We can also build a function that, given a spaCy token, gives the path till the root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-breast",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:40.987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to find the path to the root of each word in a sentence\n",
    "\n",
    "\n",
    "def path_to_the_root(token):\n",
    "    if token.head == token:\n",
    "        return\n",
    "    else:\n",
    "        print(f\"{token}->{token.head}\")\n",
    "        path_to_the_root(token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343272da",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:40.990Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_the_root(doc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-smooth",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:40.992Z"
    }
   },
   "outputs": [],
   "source": [
    "# For all roots of the tokens\n",
    "def path_to_the_root(doc):\n",
    "    for token in doc:\n",
    "        print(token.text, \" -> \", token.head)\n",
    "\n",
    "\n",
    "path_to_the_root(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e8f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d332505",
   "metadata": {},
   "source": [
    "## Embeddings \n",
    "\n",
    "An embedding is a fixed sizes numerical vector that attempts to encode some semantic meaning of the word or sentence it is encoding. The distributional hypothesis is usually the concept behind most embeddings. This hypothesis states that words which often have the same neighboring words tend to be semantically similar. For example if 'football' and 'basketball' usually appear close the word 'play' we assume that they will be semantically similar. An algorithm that is based on this concept is Word2Vec. A common way of obtaining sentence embeddings is to average the word embeddings inside the sentence and use that average as the representation of the whole sentence. \n",
    "\n",
    "- In spacy every token has its embedding.\n",
    "- It is under the attribute 'vector'.\n",
    "- In spacy embeddings are of size 96 or 128.\n",
    "\n",
    "\n",
    "Obtain the embeddings of all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee49b03",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:40.998Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919199cc",
   "metadata": {},
   "source": [
    "## Semantic similarity \n",
    "\n",
    "To compute the semantic similarity between two sentences, $u$ and $v$, we measure the cossine similarity between the two sentence embeddings. The formula is as follows:\n",
    "\n",
    "$sim(u, v) = \\frac{u \\cdot v}{||u|| ||v||} $\n",
    "\n",
    "\n",
    "Use the following formula to get the semantic similarity betwen the words in doc.\n",
    "Feel free to test it between differente words too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9421f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.001Z"
    }
   },
   "outputs": [],
   "source": [
    "def semantic_sim(u, v):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4119a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.003Z"
    }
   },
   "outputs": [],
   "source": [
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-virgin",
   "metadata": {},
   "source": [
    "# Pride and Prejudice analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-danger",
   "metadata": {},
   "source": [
    "We would like to:\n",
    "\n",
    "- Extract the names of all the characters from the book (e.g. Elizabeth, Darcy, Bingley)\n",
    "- Visualize characters' occurences with regards to relative position in the book\n",
    "- Authomatically describe any character from the book\n",
    "- Find out which characters have been mentioned in a context of marriage\n",
    "- Build keywords extraction that could be used to display a word cloud (example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-remark",
   "metadata": {},
   "source": [
    "To load the text file, it is convinient to decode using the utf-8 standard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-envelope",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.007Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-sentence",
   "metadata": {},
   "source": [
    "### Process full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-personality",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.009Z"
    }
   },
   "outputs": [],
   "source": [
    "text = read_file(\"data/pride_and_prejudice.txt\")\n",
    "# Process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-uruguay",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.012Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many sentences are in the book (Pride & Prejudice)?\n",
    "\n",
    "# Print sentences from index 10 to index 15, to make sure that we have parsed the correct book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db978f6d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of words in the book:\n",
    "\n",
    "words = text.split()\n",
    "\n",
    "print(\"Number of words in book :\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44ef44",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.021Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many sentences are in the book (Pride & Prejudice)?\n",
    "doc = nlp(text)\n",
    "sentence_tokens = [[token.text for token in sent] for sent in doc.sents]\n",
    "print(len(sentence_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63fb024",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print sentences from index 10 to index 15, to make sure that we have parsed the correct book\n",
    "print(sentence_tokens[10], sentence_tokens[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16841e3c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.029Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in range(10, 15):\n",
    "    print(sentence_tokens[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a467d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.032Z"
    }
   },
   "outputs": [],
   "source": [
    "# Other solutions for the same problems\n",
    "\n",
    "# How many sentences are in the book (Pride & Prejudice)?\n",
    "text = read_file(\"data/pride_and_prejudice.txt\")\n",
    "processed_text = nlp(text)\n",
    "\n",
    "sentences = [s for s in processed_text.sents]\n",
    "print(len(sentences))\n",
    "print(len(list(processed_text.sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093aa7a2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print sentences from index 10 to index 15, to make sure that we have parsed the correct book\n",
    "print(sentences[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-devon",
   "metadata": {},
   "source": [
    "## Find all the personal names\n",
    "\n",
    "[Hint](# \"List doc.ents and check ent.label_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-dryer",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract all the personal names from Pride & Prejudice and count their occurrences.\n",
    "# Expected output is a list in the following form: [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266) ...].\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_character_occurences(doc):\n",
    "    \"\"\"\n",
    "    Return a list of actors from `doc` with corresponding occurences.\n",
    "\n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :return: list of tuples in form\n",
    "        [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266)]\n",
    "    \"\"\"\n",
    "\n",
    "    characters = Counter()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            characters[ent.lemma_] += 1\n",
    "\n",
    "    return characters.most_common()\n",
    "\n",
    "print(find_character_occurences(processed_text)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-turtle",
   "metadata": {},
   "source": [
    "## Plot characters personal names as a time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-intranet",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.045Z"
    }
   },
   "outputs": [],
   "source": [
    "# Matplotlib Jupyter HACK\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-entity",
   "metadata": {},
   "source": [
    "We can investigate where a particular entity occurs in the text. We can do it just accessing the `.start` attribute of an entity:\n",
    "\n",
    "[Hint](# \"ent.start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-technique",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.052Z"
    }
   },
   "outputs": [],
   "source": [
    "# List all the start positions of person entities\n",
    "\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == \"PERSON\":\n",
    "        print(ent.start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-stanford",
   "metadata": {},
   "source": [
    "So we can create a function that stores all the offsets of every character:\n",
    "   \n",
    "   \n",
    "[Hint](# \"Create a dictionary with the lowered lemmas [ent.lemma_.lower()] and associate a list of all the ent.starts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-bride",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot characters' mentions as a time series relative to the position of the actor's occurrence in a book.\n",
    "def get_character_offsets(doc):\n",
    "    \"\"\"\n",
    "    For every character in a `doc` collect all the occurences offsets and store them into a list.\n",
    "    The function returns a dictionary that has actor lemma as a key and list of occurences as a value for every character.\n",
    "\n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :return: dict object in form\n",
    "        {'elizabeth': [123, 543, 4534], 'darcy': [205, 2111]}\n",
    "    \"\"\"\n",
    "\n",
    "    character_offsets = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            if ent.lemma_.lower() not in character_offsets:\n",
    "                character_offsets[ent.lemma_.lower()] = [ent.start]\n",
    "            else:\n",
    "                character_offsets[ent.lemma_.lower()].append(ent.start)\n",
    "    return dict(character_offsets)\n",
    "\n",
    "\n",
    "character_occurences = get_character_offsets(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-medium",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.064Z"
    }
   },
   "outputs": [],
   "source": [
    "character_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-companion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T17:01:35.087781Z",
     "start_time": "2021-03-28T17:01:35.071546Z"
    }
   },
   "source": [
    "[Hint](# \"Use the character offsets for each character as x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-prospect",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the histogram of the character occurrences in the whole text\n",
    "\n",
    "NUM_BINS = 20\n",
    "\n",
    "\n",
    "def plot_character_hist(character_offsets, character_label, cumulative=False):\n",
    "    x = character_offsets[character_label]\n",
    "    plt.figure()\n",
    "    n, bins, patches = plt.hist(\n",
    "        x, NUM_BINS, label=character_label, cumulative=cumulative\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-theology",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.070Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_character_hist(character_occurences, \"elizabeth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-sydney",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.073Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_character_hist(character_occurences, \"darcy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-sunset",
   "metadata": {},
   "source": [
    "### Cumulative occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-thermal",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.076Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_character_hist(character_occurences, \"elizabeth\", cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-morris",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.079Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_character_hist(character_occurences, \"darcy\", cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-senior",
   "metadata": {},
   "source": [
    "### Spacy parse tree in action\n",
    "\n",
    "[Hint](# \"ent.subtree, token.pos_ == 'ADJ'\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-camcorder",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find words (adjectives) that describe Mr. Darcy.\n",
    "\n",
    "\n",
    "def get_character_adjectives(doc, character_lemma):\n",
    "    \"\"\"\n",
    "    Find all the adjectives related to `character_lemma` in `doc`\n",
    "\n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :param character_lemma: string object\n",
    "    :return: list of adjectives related to `character_lemma`\n",
    "    \"\"\"\n",
    "\n",
    "    adjectives = []\n",
    "    for ent in processed_text.ents:\n",
    "        if ent.lemma_.lower() == character_lemma:\n",
    "            for token in ent.subtree:\n",
    "                if token.pos_ == \"ADJ\":  # Replace with if token.dep_ == 'amod':\n",
    "                    adjectives.append(token.lemma_)\n",
    "\n",
    "    for ent in processed_text.ents:\n",
    "        if ent.lemma_.lower() == character_lemma:\n",
    "            if ent.root.dep_ == \"nsubj\":\n",
    "                for child in ent.root.head.children:\n",
    "                    if child.dep_ == \"acomp\":\n",
    "                        adjectives.append(child.lemma_)\n",
    "\n",
    "    return adjectives\n",
    "\n",
    "\n",
    "print(get_character_adjectives(processed_text, \"darcy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-count",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find words (adjectives) that describe Elizabeth.\n",
    "\n",
    "print(get_character_adjectives(processed_text, \"elizabeth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-collective",
   "metadata": {},
   "source": [
    "For all the dependencies manual: https://nlp.stanford.edu/software/dependencies_manual.pdf\n",
    "\n",
    "`acomp`: adjectival complement\n",
    "*i.e.* an adjectival phrase which functions as the complement (like an object of the verb) e.g. \"She looks very beautiful\": *beautiful* is an adjectival complement of *looks*\n",
    "\n",
    "`nsubj`: nominal subject\n",
    "*i.e.* a noun phrase which is the syntactic subject of a clause. The head of this relation\n",
    "might not always be a verb: when the verb is a copular verb, the root of the clause is the complement of\n",
    "the copular verb, which can be an adjective or noun.\n",
    "*e.g.* \"Clinton defeated Dole\". The relationship is *nsubj(defeated, Clinton)*\n",
    "\n",
    "\"The baby is cute\". The relationship is *nsubj(cute, baby)*.\n",
    "\n",
    "In the code, `.dep_`stands for syntactic dependency, *i.e.* the relation between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-warrant",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.088Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_text.ents[30].root.dep_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-crown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T17:09:34.144121Z",
     "start_time": "2021-03-28T17:09:34.132840Z"
    }
   },
   "source": [
    "[Hint](# \"ent.label_, ent.root.head.lemma_\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-school",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.091Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find characters that are 'talking', 'saying', 'doing' the most. Find the relationship between \n",
    "# entities and corresponding root verbs.\n",
    "\n",
    "character_verb_counter = Counter()\n",
    "\n",
    "\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == 'PERSON' and ent.root.head.lemma_ == \"say\":\n",
    "        character_verb_counter[ent.text] += 1\n",
    "\n",
    "print(character_verb_counter.most_common(10)) \n",
    "\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == 'PERSON' and ent.root.head.lemma_ == \"do\":\n",
    "        character_verb_counter[ent.text] += 1\n",
    "\n",
    "print(character_verb_counter.most_common(10)) \n",
    "\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == 'PERSON' and ent.root.head.lemma_ == \"talk\":\n",
    "        character_verb_counter[ent.text] += 1\n",
    "\n",
    "print(character_verb_counter.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-hawaiian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T17:10:42.811139Z",
     "start_time": "2021-03-28T17:10:42.804815Z"
    }
   },
   "source": [
    "[Hint](# \"ent.label_, ent.root.head.pos_\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-organization",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find 20 most used verbs\n",
    "verb_counter = Counter()\n",
    "\n",
    "# your code here\n",
    "\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == \"PERSON\" and ent.root.head.pos_ == \"VERB\":\n",
    "        verb_counter[ent.root.head.lemma_] += 1\n",
    "\n",
    "print(verb_counter.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-familiar",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with the most used verb and how many time a character used the verb\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "verb_characters = {}\n",
    "verb_list = [verb[0] for verb in verb_counter.most_common(20)]\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == \"PERSON\" and ent.root.head.lemma_ in verb_list:\n",
    "        verb = ent.root.head.lemma_\n",
    "        person = ent.text\n",
    "        if verb not in verb_characters:\n",
    "            verb_characters[verb] = {person: 1}\n",
    "        else:\n",
    "            if ent.text not in verb_characters[verb]:\n",
    "                verb_characters[verb][person] = 1\n",
    "            else:\n",
    "                verb_characters[verb][person] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10357b0d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.101Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-election",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.108Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(verb_characters).transpose().fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-sweet",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.112Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the less meaningful columns\n",
    "df = df[df.columns[df.sum()>=10]].sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-development",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T20:37:41.118Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.heatmap(df, annot=True, cmap='Blues')\n",
    "df.style.background_gradient(cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-trace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
